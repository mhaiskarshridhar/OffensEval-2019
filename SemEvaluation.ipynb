{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\olid-training-v1.0.tsv',sep='\\t', encoding=\"utf-8\",quotechar='\\0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preparation(df):\n",
    "\n",
    "    #Convert to lower case\n",
    "    df['tweet']=df['tweet'].str.lower()\n",
    "\n",
    "    for i in range(0,len(df['tweet'])):\n",
    "        #Removal of User Id\n",
    "        df['tweet'][i] = re.sub(\"user\", \"\", df['tweet'][i])\n",
    "\n",
    "        #Removal of URL\n",
    "        df['tweet'][i] = re.sub(\"url\", \"\", df['tweet'][i])\n",
    "\n",
    "        #Converting emojis to corresponding words\n",
    "        df['tweet'][i]=emoji.demojize(df['tweet'][i], delimiters=(\"\",\" \"))\n",
    "        \n",
    "        #Removal of Punctuations\n",
    "#         df['tweet'][i] = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\\"\\’\\']\", \" \", df['tweet'][i]).split())\n",
    "#         df['tweet'] = df['tweet'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "\n",
    "        df['tweet'] = df['tweet'].str.replace(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\\"\\’\\'\\@\\+\\~]\",\"\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    #Remove words with length less the 3\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    df['tidy_tweet']=df['tweet']\n",
    "    df.update(\"'\" + df[['tweet']].astype(str) + \"'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df['subtask_c'].fillna(\"NULL\",inplace = True)\n",
    "df['subtask_b'].fillna(\"NULL\",inplace = True)\n",
    "df=preparation(df)\n",
    "# df.to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\training_updated_tweet.arff\", header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Graph for Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def graph_subtask(subtask):\n",
    "    df[subtask].value_counts(normalize=True)*100\n",
    "    sns.countplot(x=subtask, data=df)\n",
    "\n",
    "graph_subtask('subtask_a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_subtask('subtask_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_subtask('subtask_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    df_without_stopwords = pd.DataFrame(columns=['tidy_tweet','subtask_a','subtask_b','subtask_c'])\n",
    "    without_wordlist=[]\n",
    "    #NLTK stop words list\n",
    "    stop_words_list = stopwords.words('english')\n",
    "\n",
    "    #Combine wordcloud and NLTK stop words\n",
    "    stop_words = [\"will\",\"take\",\"should've\"] + list(STOPWORDS)+stop_words_list\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        querywords = df['tidy_tweet'][i].split()\n",
    "        resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "        without_wordlist.append(' '.join(resultwords))\n",
    "\n",
    "    df_without_stopwords['tidy_tweet'] = without_wordlist\n",
    "    df_without_stopwords['subtask_a']=df['subtask_a']\n",
    "    df_without_stopwords['subtask_b']=df['subtask_b']\n",
    "    df_without_stopwords['subtask_c']=df['subtask_c']\n",
    "    return df_without_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_without_stopwords=remove_stopwords(df)\n",
    "df['tidy_tweet']=df_without_stopwords['tidy_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(df_stemming,df_without_stopwords):\n",
    "    df_stemming['tokenized']=df_without_stopwords['tidy_tweet'].apply(lambda x: x.split())\n",
    "    \n",
    "def stemSentence(sentence):\n",
    "    porter = PorterStemmer()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def WordTokenize(df_stemming,df_without_stopwords):\n",
    "    df_stemming['word_tokenize']=df_without_stopwords['tidy_tweet'].apply(lambda x: stemSentence(x))\n",
    "     \n",
    "def SnowballStemmer1(df_stemming,df_without_stopwords):\n",
    "    englishStemmer=SnowballStemmer(\"english\")\n",
    "    df_stemming['SnowballStemmer123']=df_without_stopwords['tidy_tweet'].apply(lambda x: englishStemmer.stem(x))\n",
    "    \n",
    "def Porter(df_stemming,df_without_stopwords):\n",
    "    porter = PorterStemmer()\n",
    "    df_stemming['Porter']=df_without_stopwords['tidy_tweet'].apply(lambda x: porter.stem(x))\n",
    "\n",
    "def Lancaster(df_stemming,df_without_stopwords):\n",
    "    lancaster=LancasterStemmer()\n",
    "    df_stemming['Lancaster']=df_without_stopwords['tidy_tweet'].apply(lambda x: lancaster.stem(x))\n",
    "    \n",
    "def Lemmatizer(df_stemming,df_without_stopwords):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    df_stemming['Lemmatized']=df_without_stopwords['tidy_tweet'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "def StemmingAndLemma(df_stemming,df_without_stopwords):\n",
    "    #Word Tokenize using word_tokenize    \n",
    "    WordTokenize(df_stemming,df_without_stopwords)\n",
    "\n",
    "    #SnowballStemmer\n",
    "    SnowballStemmer1(df_stemming,df_without_stopwords)\n",
    "\n",
    "    #Tokenizer\n",
    "    Tokenize(df_stemming,df_without_stopwords)\n",
    "\n",
    "    #Porter Stemmer\n",
    "    Porter(df_stemming,df_without_stopwords)\n",
    "\n",
    "    #Lancaster Stemmer\n",
    "    Lancaster(df_stemming,df_without_stopwords)\n",
    "\n",
    "    #Lemmatizer \n",
    "    Lemmatizer(df_stemming,df_without_stopwords)\n",
    "\n",
    "    return df_stemming\n",
    "\n",
    "df_stemming=pd.DataFrame()\n",
    "df_stemming['tidy_tweet']=df_without_stopwords['tidy_tweet']\n",
    "\n",
    "df_stemming=StemmingAndLemma(df_stemming,df_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove words having frequency less than 3 from each Stemmed and Lematized Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  # available in Python 2.5 and newer\n",
    "\n",
    "def get_frequency_of_words(df_stemming,column_name):\n",
    "    print('Get Frequency for: ',column_name)\n",
    "    Tweet_list=df_stemming[column_name]\n",
    "    Dictionary_Tweet= defaultdict(int)\n",
    "    Tweet_word_list=[]\n",
    "    def update(i):\n",
    "        Dictionary_Tweet[i] += 1\n",
    "    [[ update(i) for i in tweet.split()] for tweet in Tweet_list]\n",
    "\n",
    "    for key,value in Dictionary_Tweet.items():\n",
    "        if value>3:\n",
    "            Tweet_word_list.append(key)\n",
    "    return Tweet_word_list\n",
    "\n",
    "def remove_words_from_tweet(df_stemming,columnname,word_list):\n",
    "    for i in range(len(df_stemming[columnname])):\n",
    "        resultwords = [word for word in df_stemming[columnname][i].split() if word in word_list]\n",
    "        if len(resultwords)!=0:\n",
    "            df_stemming[columnname][i] = ' '.join(resultwords)\n",
    "    return df_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frequency(df_stemming):\n",
    "    #Get words with minimun 3 frequency for all stemming types\n",
    "    tidy_tweet_list=get_frequency_of_words(df_stemming,'tidy_tweet')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'tidy_tweet',tidy_tweet_list)\n",
    "\n",
    "    word_tokenize_list=get_frequency_of_words(df_stemming,'word_tokenize')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'word_tokenize',word_tokenize_list)\n",
    "\n",
    "    SnowballStemmer123_list=get_frequency_of_words(df_stemming,'SnowballStemmer123')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'SnowballStemmer123',SnowballStemmer123_list)\n",
    "\n",
    "    Porter_list=get_frequency_of_words(df_stemming,'Porter')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'Porter',Porter_list)\n",
    "\n",
    "    Lancaster_list=get_frequency_of_words(df_stemming,'Lancaster')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'Lancaster',Lancaster_list)\n",
    "\n",
    "    Lemmatized_list=get_frequency_of_words(df_stemming,'Lemmatized')\n",
    "    df_stemming=remove_words_from_tweet(df_stemming,'Lemmatized',Lemmatized_list)\n",
    "    \n",
    "    return df_stemming\n",
    "df_stemming=frequency(df_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Upated Tweet's For Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet_updated']=df_stemming['tidy_tweet']\n",
    "df['word_tokenize']=df_stemming['word_tokenize']\n",
    "df['SnowballStemmer123']=df_stemming['SnowballStemmer123']\n",
    "df['Porter']=df_stemming['Porter']\n",
    "df['Lancaster']=df_stemming['Lancaster']\n",
    "df['Lemmatized']=df_stemming['Lemmatized']\n",
    "df.update(\"'\" + df[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "del df['id']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ARFF File for Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskA\\{}_subtaskA.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Train\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_a {NOT,OFF}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df[[col,'subtask_a']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskA\\{}_subtaskA.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word Cloud for Subtask A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_Wordcloud(df,columnname,subtask,target):\n",
    "    nltk.download('stopwords')\n",
    "    #NLTK stop words list\n",
    "    stop_words_list = stopwords.words('english')\n",
    "\n",
    "    #Combine wordcloud and NLTK stop words\n",
    "    stop_words = [\"will\",\"take\",\"should've\"] + list(STOPWORDS)+stop_words_list\n",
    "\n",
    "    all_words=' '.join([text for text in df[columnname][df[subtask]==target]])\n",
    "\n",
    "    wordcloud=WordCloud(stopwords = stop_words,width=1800,height=1500,max_words=100000).generate(all_words)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20,10),facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "Plot_Wordcloud(df,'tidy_tweet','subtask_a','OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate N Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = TextBlob(data).ngrams(num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def generate_ngram(df):\n",
    "    ngram_list=[]\n",
    "    ngram_subtask_a=[]\n",
    "    ngram_subtask_b=[]\n",
    "    ngram_subtask_c=[]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        querywords = df['tidy_tweet'][i].split()\n",
    "        resultwords  = [word for word in extract_ngrams(df['tidy_tweet'][i], 2)]\n",
    "\n",
    "        for words in resultwords:\n",
    "            ngram_list.append(words)\n",
    "            ngram_subtask_a.append(df['subtask_a'][i])\n",
    "            ngram_subtask_b.append(df['subtask_b'][i])\n",
    "            ngram_subtask_c.append(df['subtask_c'][i])\n",
    "\n",
    "    df_ngram=pd.DataFrame()\n",
    "    df_ngram['ngram']=ngram_list\n",
    "    df_ngram['subtask_a']=ngram_subtask_a\n",
    "    df_ngram['subtask_b']=ngram_subtask_b\n",
    "    df_ngram['subtask_c']=ngram_subtask_c\n",
    "\n",
    "    return df_ngram\n",
    "\n",
    "df_ngram=generate_ngram(df)\n",
    "\n",
    "df_ngram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Tweets for Subtask B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtask_b=pd.DataFrame()\n",
    "df_subtask_b = df.ix[(df['subtask_a'] == 'OFF'),['tidy_tweet','subtask_a','subtask_b','subtask_c']]\n",
    "df_subtask_b = df_subtask_b.reset_index()\n",
    "del df_subtask_b['index']\n",
    "df_subtask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtask_b['tidy_tweet']=df_subtask_b['tidy_tweet'].str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization for Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemming_b=pd.DataFrame()\n",
    "df_stemming_b['tidy_tweet']=df_subtask_b['tidy_tweet']\n",
    "\n",
    "df_stemming_b=StemmingAndLemma(df_stemming_b,df_subtask_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove words with frequency less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemming_b=frequency(df_stemming_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Updated Tweet's for Subtask_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtask_b['tidy_tweet_updated']=df_stemming_b['tidy_tweet']\n",
    "df_subtask_b['word_tokenize']=df_stemming_b['word_tokenize']\n",
    "df_subtask_b['SnowballStemmer123']=df_stemming_b['SnowballStemmer123']\n",
    "df_subtask_b['Porter']=df_stemming_b['Porter']\n",
    "df_subtask_b['Lancaster']=df_stemming_b['Lancaster']\n",
    "df_subtask_b['Lemmatized']=df_stemming_b['Lemmatized']\n",
    "\n",
    "df_subtask_b.update(\"'\" + df_subtask_b[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "df_subtask_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ARFF File for Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskB\\{}_subtaskB.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Train\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_b {UNT,TIN}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df_subtask_b[[col,'subtask_b']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskB\\{}_subtaskB.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud for Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Wordcloud(df_subtask_b,'tidy_tweet','subtask_b','TIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Tweets for Subtask C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtask_c=pd.DataFrame()\n",
    "df_subtask_c = df_subtask_b.ix[(df_subtask_b['subtask_b'] == 'TIN'),['tidy_tweet','subtask_a','subtask_b','subtask_c']]\n",
    "df_subtask_c = df_subtask_c.reset_index()\n",
    "del df_subtask_c['index']\n",
    "\n",
    "df_subtask_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtask_c['tidy_tweet']=df_subtask_c['tidy_tweet'].str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization for Subtask C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemming_c=pd.DataFrame()\n",
    "df_stemming_c['tidy_tweet']=df_subtask_c['tidy_tweet']\n",
    "\n",
    "df_stemming_c=StemmingAndLemma(df_stemming_c,df_subtask_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove words with frequency less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stemming_c=frequency(df_stemming_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Updated Tweet's for Subtask_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtask_c['tidy_tweet_updated']=df_stemming_c['tidy_tweet']\n",
    "df_subtask_c['word_tokenize']=df_stemming_c['word_tokenize']\n",
    "df_subtask_c['SnowballStemmer123']=df_stemming_c['SnowballStemmer123']\n",
    "df_subtask_c['Porter']=df_stemming_c['Porter']\n",
    "df_subtask_c['Lancaster']=df_stemming_c['Lancaster']\n",
    "df_subtask_c['Lemmatized']=df_stemming_c['Lemmatized']\n",
    "df_subtask_c.update(\"'\" + df_subtask_c[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "\n",
    "df_subtask_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ARFF File for Subtask C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskC\\{}_subtaskC.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Train\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_c {IND,OTH,GRP}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df_subtask_c[[col,'subtask_c']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskC\\{}_subtaskC.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud for Subtask C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Plot_Wordcloud(df_subtask_c,'tidy_tweet','subtask_c','IND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# **********  Test File  **********\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_a=pd.read_csv(r'C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\test_a.csv',sep=',', encoding=\"utf-8\",quotechar='\\0')\n",
    "df_test_b=pd.read_csv(r'C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\test_b.csv',sep=',', encoding=\"utf-8\",quotechar='\\0')\n",
    "df_test_c=pd.read_csv(r'C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\test_c.csv',sep=',', encoding=\"utf-8\",quotechar='\\0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_a=preparation(df_test_a)\n",
    "df_test_b=preparation(df_test_b)\n",
    "df_test_c=preparation(df_test_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_test(df_test):\n",
    "    nltk.download('stopwords')\n",
    "    without_wordlist=[]\n",
    "    #NLTK stop words list\n",
    "    stop_words_list = stopwords.words('english')\n",
    "\n",
    "    #Combine wordcloud and NLTK stop words\n",
    "    stop_words = [\"will\",\"take\",\"should've\"] + list(STOPWORDS)+stop_words_list\n",
    "\n",
    "    for i in range(len(df_test)):\n",
    "        querywords = df_test['tidy_tweet'][i].split()\n",
    "        resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "        without_wordlist.append(' '.join(resultwords))\n",
    "\n",
    "    df_test['tidy_tweet'] = without_wordlist\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_a=remove_stopwords_test(df_test_a)\n",
    "df_test_b=remove_stopwords_test(df_test_b)\n",
    "df_test_c=remove_stopwords_test(df_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set A\n",
    "df_stemming_test_a=pd.DataFrame()\n",
    "df_stemming_test_a['tidy_tweet']=df_test_a['tidy_tweet']\n",
    "df_stemming_test_a=StemmingAndLemma(df_stemming_test_a,df_test_a)\n",
    "\n",
    "\n",
    "#Test Set B\n",
    "df_stemming_test_b=pd.DataFrame()\n",
    "df_stemming_test_b['tidy_tweet']=df_test_b['tidy_tweet']\n",
    "df_stemming_test_b=StemmingAndLemma(df_stemming_test_b,df_test_b)\n",
    "\n",
    "#Test Set C\n",
    "\n",
    "df_stemming_test_c=pd.DataFrame()\n",
    "df_stemming_test_c['tidy_tweet']=df_test_c['tidy_tweet']\n",
    "df_stemming_test_c=StemmingAndLemma(df_stemming_test_c,df_test_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_a['tidy_tweet_updated']=df_stemming_test_a['tidy_tweet']\n",
    "df_test_a['word_tokenize']=df_stemming_test_a['word_tokenize']\n",
    "df_test_a['SnowballStemmer123']=df_stemming_test_a['SnowballStemmer123']\n",
    "df_test_a['Porter']=df_stemming_test_a['Porter']\n",
    "df_test_a['Lancaster']=df_stemming_test_a['Lancaster']\n",
    "df_test_a['Lemmatized']=df_stemming_test_a['Lemmatized']\n",
    "df_test_a.update(\"'\" + df_test_a[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "del df_test_a['id']\n",
    "del df_test_a['tweet']\n",
    "df_test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_b['tidy_tweet_updated']=df_stemming_test_b['tidy_tweet']\n",
    "df_test_b['word_tokenize']=df_stemming_test_b['word_tokenize']\n",
    "df_test_b['SnowballStemmer123']=df_stemming_test_b['SnowballStemmer123']\n",
    "df_test_b['Porter']=df_stemming_test_b['Porter']\n",
    "df_test_b['Lancaster']=df_stemming_test_b['Lancaster']\n",
    "df_test_b['Lemmatized']=df_stemming_test_b['Lemmatized']\n",
    "df_test_b.update(\"'\" + df_test_b[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "del df_test_b['id']\n",
    "del df_test_b['tweet']\n",
    "df_test_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_c['tidy_tweet_updated']=df_stemming_test_c['tidy_tweet']\n",
    "df_test_c['word_tokenize']=df_stemming_test_c['word_tokenize']\n",
    "df_test_c['SnowballStemmer123']=df_stemming_test_c['SnowballStemmer123']\n",
    "df_test_c['Porter']=df_stemming_test_c['Porter']\n",
    "df_test_c['Lancaster']=df_stemming_test_c['Lancaster']\n",
    "df_test_c['Lemmatized']=df_stemming_test_c['Lemmatized']\n",
    "df_test_c.update(\"'\" + df_test_c[['tidy_tweet','tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']].astype(str) + \"'\")\n",
    "del df_test_c['id']\n",
    "del df_test_c['tweet']\n",
    "df_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_c['subtask_c_weka']='?'\n",
    "df_test_b['subtask_b_weka']='?'\n",
    "df_test_a['subtask_a_weka']='?'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ARFF Files for Test Set A, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskA_TEST\\{}_subtaskA_Test.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Test\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_a {NOT,OFF}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df_test_a[[col,'subtask_a']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskA_TEST\\{}_subtaskA_Test.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskB_TEST\\{}_subtaskB_Test.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Test\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_b {UNT,TIN}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df_test_b[[col,'subtask_b']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskB_TEST\\{}_subtaskB_Test.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=['tidy_tweet_updated','word_tokenize','SnowballStemmer123','Porter','Lancaster','Lemmatized']\n",
    "\n",
    "for col in column:\n",
    "    f= open(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskC_TEST\\{}_subtaskC_Test.arff\".format(col),\"a+\")\n",
    "    f.write(\"@relation Test\\n\")\n",
    "    f.write(\"@attribute {} string\\n\".format(col))\n",
    "    f.write(\"@attribute subtask_c {IND,GRP,OTH}\\n\")\n",
    "    f.write(\"@data\\n\")\n",
    "    f.close()\n",
    "    df_test_c[[col,'subtask_c']].to_csv(r\"C:\\Users\\Mhaiskao\\Desktop\\Assignments\\DMTA\\Maverick\\SubTaskC_TEST\\{}_subtaskC_Test.arff\".format(col), header=False,index=False,mode='a',quotechar=\"'\",quoting=3,escapechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
